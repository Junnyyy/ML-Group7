---
title: "Report"
header-includes:
 \usepackage{readr}
output: word_document
date: "2022-11-21"
---
# Introduction  
Response Variable: 

* stroke:  

  
Possible Predictors: 

* smoking_status: The smoking status of the observation. Factor variable with 3 levels: "formerly smoked", "never smoked", and "smokes".  


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
## Cleaning the data  
To use this data set, we first converted variables to their correct representations and then omitted any incomplete observations.  

### Notable fixes: 
* The categorical variable smoking_status was reformatted to remove the level "Unknown", as it was used to represent unavailable data. Any cells that previously had "Unknown" were updated to reflect their unavailable status. 
* The quantitative variable bmi was fixed by converting the data type from character to numeric. This change does not apply to non-numeric cells, so the cells containing "N/A" were changed to reflect their unavailable status in the now numeric column.  
```{r, include=FALSE}
library(readr)
# Load in health Data
data <- read_csv("C:/Users/Alyssa Guillory/Desktop/MATH 4322/GROUP PROJ/Project/ML-Group7/healthcare-dataset-stroke-data.csv", col_types = cols(gender = col_factor(levels = c("Male", "Female", "Other")), hypertension = col_factor(levels = c("0", "1")), heart_disease = col_factor(levels = c("0", "1")), ever_married = col_factor(levels = c("No", "Yes")), work_type = col_factor(levels = c("children", "Govt_job", "Never_worked", "Private", "Self-employed")), Residence_type = col_factor(levels = c("Rural", "Urban")), bmi = col_number(), smoking_status = col_factor(levels = c("formerly smoked", "never smoked", "smokes")), stroke = col_factor(levels = c("0", "1"))), na = c("Unknown", "N/A", ""))
# Remove ID column
health = data[,-1]
# Omit NA values
health = na.omit(health)
```  

---**still need to list the rest of the variables, short description of data, and the question we want to answer**

# Logistic Regression Model  

We chose to use a logistic regression model for our data because our response variable is qualitative with two classes. It was desirable to have a model predict the probability of a person having a stroke using binary classification. If linear regression is used to predict the probability of whether or not a person will have a stroke, then the model may have predicted Y values outside of our intended range of 0-1. The logistic regression model allows us to enforce this restriction of range.  

## Model Formula   
Our basic logistic regression formula with all our predictors would be as follows:  

```{r log reg model formula, out.width = '100%'}
knitr::include_graphics("logreg.png")
```  

```{r, eval=FALSE, include=FALSE}
library(ggplot2)
library(GGally)
ggpairs(health[,c(2:4,8,11)])
```  

This is what our logistic regression formula would look like if we considered all of our predictors and the response variable being stroke. However, we automatically did not consider the unique identifier (id) as this variable is only used to identify patients, which does not have a significant influence on predicting the probability of strokes. Using the glm() function, we created our initial logistic model with stroke being the response and all the other variables as predictors (except id).

```{r,include=FALSE}
#create model with all predictors
set.seed(500)
health.glm = glm(stroke ~ ., family = "binomial", data=health)
#create model with significant predictors found from doing stepwise backward method on original model
health2.glm = step(health.glm, direction = "backward")
#health2.glm = glm(formula = stroke ~ age + hypertension + heart_disease + avg_glucose_level, 
#    family = "binomial", data = health)

#create a model w/out heart_disease
health3.glm = glm(formula = stroke ~ age + hypertension + avg_glucose_level, family = "binomial", data = health)
```  
```{r}
summary(health.glm)
```  

As the summary shows, the predictors age, hypertension, and avg_glucose_level show high levels of significance, and the predictor heart_disease show moderate levels of significance. Since our p-values for all four predictors are less than 0.1, we can confidently reject the null hypothesis that states H0: B0 = B1= …= Bn. To confirm that these variables are significant, we used the backwards step() function to see if it would give us the same significant variables, and it did.
```{r,echo=TRUE}
#Model created from backwards step()
summary(health2.glm)
#Model created without heart_disease
summary(health3.glm)
```  
By removing the predictors that had little to no significance in predicting the response variable, we are able to refit our data into a second model (health2.glm) with the four predictors that are significant. The original model with all predictors had an AIC of 1173.8, while the second model with the four significant predictors had an AIC of 1159.9. As there was not much of a difference between the first and second model, we decided to remove the lowest significant predictor (heart_disease) out of the four to perform one last model refitting of our data. This time, our AIC for the third model (health3.glm) came out to be 1162.1, which was a slight increase compared to the second model.

```{r, include=FALSE}
#comparing models
#reminder: table w/null dev, residual dev, r^2, AIC for each model

#health.glm info
summary(health.glm)
fit.bic=BIC(health.glm)
fit.stat = cbind(health.glm$null.deviance,health.glm$deviance,(1-(health.glm$deviance/health.glm$null.deviance)),health.glm$aic,fit.bic)
colnames(fit.stat) = c("Null Deviance","Residual Deviance","R^2","AIC","BIC")
#print(fit.stat)
#R^2: 1-(1141.8/1411) =0.1907867

#health2.glm info
summary(health2.glm)
health2.bic=BIC(health2.glm)
health2.stat = cbind(health2.glm$null.deviance,health2.glm$deviance,(1-(health2.glm$deviance/health2.glm$null.deviance)),health2.glm$aic,health2.bic)
colnames(health2.stat) = c("Null Deviance","Residual Deviance","R^2","AIC","BIC")
#print(health2.stat)
#R^2: 1-(1149/1411) =0.1856839

#health3.glm info
summary(health3.glm)
health3.bic=BIC(health3.glm)
health3.stat = cbind(health3.glm$null.deviance,health3.glm$deviance,(1-(health3.glm$deviance/health3.glm$null.deviance)),health3.glm$aic, health3.bic)
colnames(fit.stat) = c("Null Deviance","Residual Deviance","R^2","AIC","BIC")
#print(health3.stat)
#R^2: 1-(1154.1/1411) = 0.1820695

goodness.fit=rbind(fit.stat,health2.stat,health3.stat)
rownames(goodness.fit) = c("health.glm","health2.glm","health3.glm")
#some info for explaining why we're picking the predictors from health2.glm:
#lower residual dev, higher R^2, and lower AIC for health2.glm shows that including the heart_disease predictor helps to create a better model


#logistic reg. coefficients for equation:

#Intercept:-7.632810
#Age: 0.067773
#hypertension1:0.568379
#heart_disease1:0.453704
#avg_glucose_level: 0.004701
```  
Additional information was obtained by using the Goodness of Fit statistical hypothesis test. Given the three models we tested above, we are able to determine each models’ null deviance, residual deviance, R^2 value, AIC value, and BIC value. The lower the residual deviance, AIC, and BIC values, the better the model is able to predict the value of the response variable. As for R^2, a value closest to 1 is best.  

```{r}
print(goodness.fit)
```  
Between all three models, better.health and best.health provided the closest fit for our data, with better.health having a slight advantage in lower residual deviance and AIC values. The test results show that each model's R^2 value is extremely low, with values being between 0.1 and 0.2, informing us that only 10%-20% of the variance can be explained by our models. This means that using logistics regression to fit our data may not result in the best fitting model. Even so, we can conclude that the model better.health (with predictors age, hypertension1, heart_disease1, and avg_glucose_level) had the best fit compared to the other models and is somewhat accurate in predicting whether or not a stroke will occur.

```{r, include=FALSE}
##training & testing
test.error = 0
train.error = 0
test.sensitivity = 0
test.specificity = 0
for (i in 1:10) {
  set.seed(i)
  #Select 80% of the data
  sample = sample.int(n = nrow(health),size = round(.80*nrow(health)), replace = FALSE)
  
  train = health[sample,]
  test = health[-sample,]
  #create model on training data
  health.train = glm(formula = stroke ~ age + hypertension + heart_disease + avg_glucose_level, family = "binomial", data = train)
  #summary(health.train)
  
  #predicting based on the model fit with the training data
  prediction.train = predict(health.train, type="response")
  predict.stroke.train = ifelse(prediction.train < 0.5,"0","1")
  (conf.mat.train = table(predict.stroke.train,train$stroke))
  
  rows.train = nrow(conf.mat.train)
  if(rows.train < 2) {
    train.error[i] = (conf.mat.train[1,2]/sum(conf.mat.train))
  } else {
    train.error[i] = (conf.mat.train[1,2]+conf.mat.train[2,1])/sum(conf.mat.train)
  }
  
  #predicting based on the model fit with the testing data
  predict.health = predict(health.train, type = "response", newdata = test)
  predict.stroke = ifelse(predict.health< 0.5,"0","1")
  (conf.test = table(predict.stroke,test$stroke))
  
  rows = nrow(conf.test)
  if(rows < 2) {
    test.error[i] = (conf.test[1,2]/sum(conf.test))
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/conf.test[1,1]
  } else if(ncol(conf.test)<1) {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
  else {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = conf.test[2,2]/(conf.test[1,2]+conf.test[2,2])
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
}
```  

```{r, echo=TRUE}
#Mean of the train error rate
(train.error.mean = mean(train.error))
#Mean of the test error
(test.error.mean = mean(test.error))
#Test Sensitivity
test.sensitivity
(test.sensitivity.mean = mean(test.sensitivity))
#Test Specificity
test.specificity
(test.specificity.mean = mean(test.specificity))
#Training data - observations used to train or teach the method how to estimate f.
#Test data - observations used to determine the accuracy of estimating f.
```  


explain model choice  
write model equation  
test/train models  
test error rate  
interpretation of full model in regards to our question: which factors demonstrate statistical significance in relation to having a stroke?  

# Conclusion
