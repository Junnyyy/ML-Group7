---
title: "Stroke Analysis Report"
author: "Christine Do, Alyssa Guillory, Johnny Le, Giselle Ruiz, Christopher Turcios, Jimmy Vuong"
date: "2022-12-02"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
```   

\newpage

## Introduction  
With heart disease being the leading cause of death in the United States, we think it’s important to examine the factors that may lead to strokes. Examining the factors for strokes helps predict and create an image of strokes and heart disease. Our data is sourced from Kaggle with a confidential source and is designated for educational purposes only.

### Dataset Information  
Data Variables:   

-   smoking_status: The smoking status of the observation. [Categorical factor w/3 levels]
-   id: The unique identifier for the given observation [numeric]
-   gender: The gender of the observation [Categorical Factor w/3 levels]
-   age: Age of the observation [numeric]
-   hypertension: Whether or not the observation has hypertension [Binary dummy variable]
-   heart_disease: Whether or not the observation has heart disease [Binary dummy variable]
-   ever_married: Whether or not the observation is married [Binary dummy variable]
-   work_type: The type of work the observation participates in [Categorical Factor w/5 levels]
-   Residence_type: The location in which the observation is located [Categorical factor w/2 levels]
-   avg_glucose_level: The average glucose level [numeric]
-   bmi: The body mass index [numeric]
-   smoking_status: The smoking status of the observation. [Categorical factor w/3 levels]
-   stroke: If the observation has had a stroke [Binary dummy variable]

#### Cleaning the data
To use this data set, we first converted variables to their correct representations and then omitted any incomplete observations.

##### Notable fixes:
-   The categorical variable smoking_status was reformatted to remove the level "Unknown", as it was used to represent unavailable data. Any cells that previously had "Unknown" were updated to reflect their unavailable status.
-   The quantitative variable bmi was fixed by converting the data type from character to numeric. This change does not apply to non-numeric cells, so the cells containing "N/A" were changed to reflect their unavailable status in the now numeric column.

```{r, include=FALSE}
library(readr)
# Load in health Data
health <- read_csv("healthcare-dataset-stroke-data.csv",
  col_types = cols(
    gender = col_factor(levels = c("Male", "Female", "Other")),
    hypertension = col_factor(levels = c("0", "1")),
    heart_disease = col_factor(levels = c("0", "1")),
    ever_married = col_factor(levels = c("No", "Yes")),
    work_type = col_factor(
      levels =
        c("children", "Govt_job", "Never_worked", "Private", "Self-employed")
    ),
    Residence_type = col_factor(levels = c("Rural", "Urban")),
    bmi = col_number(),
    smoking_status = col_factor(
      levels =
        c("formerly smoked", "never smoked", "smokes")
    ),
    stroke = col_factor(levels = c("0", "1"))
  ),
  na = c("Unknown", "N/A", "")
)
# Remove ID column
health = health[,-1]
# Omit NA values
health = na.omit(health)
```   
### Main Question
Which factors demonstrate statistical significance in relation to the response variable stroke?

\newpage

## Logistic Regression (Christine Do, Alyssa Guillory, Jimmy Vuong)
We chose to use a logistic regression model for our data because our response variable is qualitative with two classes. It was desirable to have a model predict the probability of a person having a stroke using binary classification. If linear regression had been used to predict the probability of whether or not a person will have a stroke instead, then the model may have predicted Y values outside of our intended range of 0-1. The logistic regression model allows us to enforce this restriction of range.

### Model Formula
Our basic logistic regression formula with all our predictors would be as follows:

```{r log reg model formula, out.width = '100%'}
knitr::include_graphics("logreg.png")
```   

This is the logistic regression formula when we consider all of our variables as predictors for the response variable stroke. However, we automatically excluded id as a predictor, as it is a unique number arbitrarily used to identify patients, therefore we know it does not influence our response variable. Using the glm() function, we created our initial logistic model with stroke being the response and all the other variables as predictors.  
```{r,include=FALSE}
#create model with all predictors
set.seed(500)
health.glm = glm(stroke ~ ., family = "binomial", data=health)
#create model with significant predictors found from doing stepwise backward method on original model
health2.glm = step(health.glm, direction = "backward")
#health2.glm = glm(formula = stroke ~ age + hypertension + heart_disease + avg_glucose_level, 
#    family = "binomial", data = health)

#create a model w/out heart_disease
health3.glm = glm(formula = stroke ~ age + hypertension + avg_glucose_level, family = "binomial", data = health)
```   
```{r}
summary(health.glm)
```  
As the summary for health.glm shows, the predictors: age, hypertension, and avg_glucose_level, show high levels of significance and the predictor heart_disease shows a moderate level of significance. Since our p-values for all four of these predictors are less than $\alpha$=0.1, we believe that these are the most important variables in predicting the probability of having a stoke; To confirm that these variables are the most significant, we used the backwards step() function to see if it would give us the same significant variables and it did.   
```{r,echo=TRUE}
#Model created from backwards step()
summary(health2.glm)
#Model created without heart_disease
summary(health3.glm)
```

From the health2.glm summary we can see that the AIC is 1159.9, which is an improvement from health.glm's AIC of 1173.8, but not by much. With that in mind, we decided to remove the lowest significant predictor (heart_disease) to perform one last model refitting on our data for further comparison. This time, our AIC for the third model (health3.glm) came out to be 1162.1, which is worse than health2.glm, but still slightly better than health.glm.   
$$P(stroke = 1) =$$

$$\frac{exp(-7.632810+0.067773*age+0.568379*hypertension+0.453704*heart\_disease+0.004701*avg\_glucose\_level)}{1 + exp(-7.632810+0.067773*age+0.568379*hypertension+0.453704*heart\_disease+0.004701*avg\_glucose\_level)}$$   
```{r, include=FALSE}
#comparing models

#health.glm info
summary(health.glm)
fit.bic=BIC(health.glm)
fit.stat = cbind(health.glm$null.deviance,health.glm$deviance,(1-(health.glm$deviance/health.glm$null.deviance)),health.glm$aic,fit.bic)
colnames(fit.stat) = c("Null Deviance","Residual Deviance","R^2","AIC","BIC")

#health2.glm info
summary(health2.glm)
health2.bic=BIC(health2.glm)
health2.stat = cbind(health2.glm$null.deviance,health2.glm$deviance,(1-(health2.glm$deviance/health2.glm$null.deviance)),health2.glm$aic,health2.bic)
colnames(health2.stat) = c("Null Deviance","Residual Deviance","R^2","AIC","BIC")

#health3.glm info
summary(health3.glm)
health3.bic=BIC(health3.glm)
health3.stat = cbind(health3.glm$null.deviance,health3.glm$deviance,(1-(health3.glm$deviance/health3.glm$null.deviance)),health3.glm$aic, health3.bic)
colnames(fit.stat) = c("Null Deviance","Residual Deviance","R^2","AIC","BIC")

goodness.fit=rbind(fit.stat,health2.stat,health3.stat)
rownames(goodness.fit) = c("health.glm","health2.glm","health3.glm")
```

Additional information was obtained by using the Goodness of Fit statistical hypothesis test. Given the three models we tested above, we are able to determine each models' null deviance, residual deviance, R\^2 value, AIC value, and BIC value. The lower the residual deviance, AIC, and BIC values, the better the model is able to predict the value of the response variable. As for R\^2, a value closest to 1 is best.

```{r}
print(goodness.fit)
```

```{r, include=FALSE}
##training & testing
test.error = 0
train.error = 0
test.sensitivity = 0
test.specificity = 0
for (i in 1:10) {
  set.seed(i)
  #Select 80% of the data
  sample = sample.int(n = nrow(health),size = round(.80*nrow(health)), replace = FALSE)
  
  train = health[sample,]
  test = health[-sample,]
  #create model on training data
  health.train = glm(formula = stroke ~ age + hypertension + heart_disease + avg_glucose_level, family = "binomial", data = train)
  #summary(health.train)
  
  #predicting based on the model health with the training data
  prediction.train = predict(health.train, type="response")
  predict.stroke.train = ifelse(prediction.train < 0.5,"0","1")
  (conf.mat.train = table(predict.stroke.train,train$stroke))
  
  rows.train = nrow(conf.mat.train)
  if(rows.train < 2) {
    train.error[i] = (conf.mat.train[1,2]/sum(conf.mat.train))
  } else {
    train.error[i] = (conf.mat.train[1,2]+conf.mat.train[2,1])/sum(conf.mat.train)
  }
  
  #predicting based on the model fit with the testing data
  predict.health = predict(health.train, type = "response", newdata = test)
  predict.stroke = ifelse(predict.health< 0.5,"0","1")
  (conf.test = table(predict.stroke,test$stroke))
  
  rows = nrow(conf.test)
  if(rows < 2) {
    test.error[i] = (conf.test[1,2]/sum(conf.test))
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/conf.test[1,1]
  } else if(ncol(conf.test)<1) {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
  else {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = conf.test[2,2]/(conf.test[1,2]+conf.test[2,2])
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
}
train.error.m = mean(train.error)
test.error.m = mean(test.error)
test.sensitivity.m = mean(test.sensitivity)
test.specificity.m = mean(test.specificity)

```   

```{r, echo=FALSE}
cat("Train Error Rate Mean: ", train.error.m)
cat("Test Error Rate Mean: ", test.error.m)
cat("Test Sensitivity Mean: ", test.sensitivity.m)
cat("Test Specificity Mean: ", test.specificity.m)
```  
Between the three models, health2.glm and health3.glm provided the closest fit for our data, with health2.glm having a slight advantage with lower residual deviance and AIC values. Each model's R\^2 value is extremely low, with values being between 0.1 and 0.2, indicating that these models are not a good fit for our data. Even so, we conclude that the best linear regression model for fitting the data and predicting whether or not a stroke will occur is the model health2.glm (predictors: age, hypertension, heart_disease, avg_glucose_level). 

\newpage

## Decision Tree (Johnny Le, Giselle Ruiz, Christopher Turcios)

For our second model, we decided to explore using a tree-based model. More specifically, we decided on a classification decision tree, once again due to our response variable being qualitative with two classes. Using a tree-based model would allow us to more easily visualize the probability of a person having a stroke, all based on our predictor variables. Furthermore, with a classification tree, we would be able to perform multiple actions, such as pruning, bagging, and random forest, to improve our model and its’ error rate.

```{r, message=FALSE, include=FALSE}
#install.packages("tree", repos = "http://cran.us.r-project.org")
#install.packages("randomForest", repos = "http://cran.us.r-project.org")
library(tree)
library(randomForest)
```

### Base Decision Tree

To create a baseline for all of our decision tree based models we created an base decision tree using `tree()` include all data used by our logistic regression model. 

```{r, out.width="60%", fig.align='center'}
# Test random forest model 10 times
test.error = 0
train.error = 0
test.sensitivity = 0
test.specificity = 0

for (i in 1:10) {
  set.seed(i)

  train <- sample(1:nrow(health), nrow(health) / 2 + 0.5)
  health.train <- health[train, ]

  # Create testing data
  health.test <- health[-train, ]


  # Create Decision Tree
  health.tree <- tree(stroke ~ ., data = health.train)

  # Predict using testing Data
  health.pred <- predict(health.tree, health.test, type = "class")
  (conf.test = table(health.pred, health.test$stroke))
  
  rows = nrow(conf.test)
  if(rows < 2) {
    test.error[i] = (conf.test[1,2]/sum(conf.test))
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/conf.test[1,1]
  } else if(ncol(conf.test)<1) {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
  else {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = conf.test[2,2]/(conf.test[1,2]+conf.test[2,2])
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
}

cat("Test Error: ", mean(test.error))
cat("Test Sensitivity: ", mean(test.sensitivity))
cat("Test Specificity: ", mean(test.specificity))

plot(health.tree)
text(health.tree, pretty = 0)
```

As the summary above shows there is an extremely low Test Sensitivity, leading us to have low confidence in the tree. Rather than the tree having many factors the model chooses a small set of factors which is an uncommon result. To cover all bases we used pruning to determine if it would provide us with a different result.

\newpage

### Pruned Decision Tree

```{r, out.width="60%", fig.align='center'}
# Test random forest model 10 times
test.error = 0
train.error = 0
test.sensitivity = 0
test.specificity = 0

for (i in 1:10) {
  set.seed(i)

  train <- sample(1:nrow(health), nrow(health) / 2 + 0.5)
  health.train <- health[train, ]

  # Create testing data
  health.test <- health[-train, ]


  # Prune Decision Tree
  cv.health <- cv.tree(health.tree, FUN = prune.misclass)

  # Predict using Pruned Tree
  health.pruned <- prune.misclass(health.tree, best = 5)
  health.pred.pruned <- predict(health.pruned, health.test, type = "class")
  (conf.test = table(health.pred.pruned, health.test$stroke))
  
  rows = nrow(conf.test)
  if(rows < 2) {
    test.error[i] = (conf.test[1,2]/sum(conf.test))
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/conf.test[1,1]
  } else if(ncol(conf.test)<1) {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
  else {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = conf.test[2,2]/(conf.test[1,2]+conf.test[2,2])
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
}

cat("Test Error: ", mean(test.error))
cat("Test Sensitivity: ", mean(test.sensitivity))
cat("Test Specificity: ", mean(test.specificity))

plot(cv.health$size, cv.health$dev, type = "b")
```

As seen from the summary above the pruned tree provides quite a different result from normal expectations. A Test Sensitivity of $0$ would mean that the model entirely fails to predict any occurances of stroke. This would make the pruned model unfit. While this deviates from the norm, it is understable. The base decision tree model had an extremely low Test Sensitivity and any pruning attempts would lower fitment towards our data. As pruning has shown to be unfit for a model we create a bagged tree using `randomForest()`.

\newpage

### Bagged Decision Tree

```{r, out.width="60%", fig.align='center'}
# Test random forest model 10 times
test.error = 0
train.error = 0
test.sensitivity = 0
test.specificity = 0

for (i in 1:10) {
  set.seed(i)

  train <- sample(1:nrow(health), nrow(health) / 2 + 0.5)
  health.train <- health[train, ]

  # Create testing data
  health.test <- health[-train, ]


  # Bagged Model
  p <- ncol(health)
  n <- nrow(health)
  B <- 1000
  health.bag <- randomForest(stroke ~ ., data = health.train, mtry = p, ntree = B, importance = TRUE)

  # Predict using Bagged Model
  health.pred.bag <- predict(health.bag, health.test, type = "class")
  (conf.test = table(health.pred.bag, health.test$stroke))
  
  rows = nrow(conf.test)
  if(rows < 2) {
    test.error[i] = (conf.test[1,2]/sum(conf.test))
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/conf.test[1,1]
  } else if(ncol(conf.test)<1) {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
  else {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = conf.test[2,2]/(conf.test[1,2]+conf.test[2,2])
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
}

cat("Test Error: ", mean(test.error))
cat("Test Sensitivity: ", mean(test.sensitivity))
cat("Test Specificity: ", mean(test.specificity))

health.tree.bag <- tree(stroke ~ ., data = health.train, subset = health.bag$inbag)
plot(health.tree.bag)
text(health.tree.bag, pretty = 0)
```

The bagged model provides the best result thus far. However, the model still suffers from an incredible low Test Sensitivity of only $0.02$ implying the model can only accurate predict a small portion of strokes. 

```{r, out.width="60%", fig.align='center'}
varImpPlot(health.bag)
```

As shown on the importance table BMI has an significant impact on the the occurance of stroke. The decision tree does not reflect this result. Differing from the logisitic model the table indicates low impact from Heart Disease, which contradicts common census that Heart Disease has an impact on stroke.

\newpage

### Random Forest

```{r, out.width="60%", fig.align='center'}
# Test random forest model 10 times
test.error = 0
train.error = 0
test.sensitivity = 0
test.specificity = 0

for (i in 1:10) {
  set.seed(i)

  train <- sample(1:nrow(health), nrow(health) / 2 + 0.5)
  health.train <- health[train, ]

  # Create testing data
  health.test <- health[-train, ]


  # Random Forest Variables
  STROKE.test <- as.factor(health.train$stroke)
  X.test <- health[-train, -11]
  p <- ncol(health)
  n <- nrow(health)
  B <- 1000

  # Random Forest Model
  health.rf <- randomForest(stroke ~ ., data = health, subset = train, xtest = X.test, ytest = STROKE.test, mtry = sqrt(p), ntree = B, importance = TRUE, keep.forest = TRUE)

  # Predict using Random Forest Model
  health.pred.rf <- predict(health.rf, health.test, type = "class")
  health.stroke.pred <- ifelse(health.pred.rf < 0.5, "0", "1")
  (conf.test = table(health.pred.rf, health.test$stroke))
  
  rows = nrow(conf.test)
  if(rows < 2) {
    test.error[i] = (conf.test[1,2]/sum(conf.test))
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/conf.test[1,1]
  } else if(ncol(conf.test)<1) {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
  else {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = conf.test[2,2]/(conf.test[1,2]+conf.test[2,2])
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
}

cat("Test Error: ", mean(test.error))
cat("Test Sensitivity: ", mean(test.sensitivity))
cat("Test Specificity: ", mean(test.specificity))
```

The Random Forest model provides a slightly different result from our Bagged model. While there is a $0.002$ decrease in Test Error this is accompanied by a $0.012$ decrease in Test Sensitivity. The decrease in Test Sensitivity is relatively small in scheme of prediction, however, in this situation it makes the Random Forest model worse than our Bagged model.

```{r, out.width="60%", fig.align='center'}
varImpPlot(health.rf)
```

```{r, out.width="60%", fig.align='center'}
health.tree.rf <- tree(stroke ~ ., data = health.train, subset = health.rf$inbag)
plot(health.tree.rf)
text(health.tree.rf, pretty = 0)
```

Between all of our models the Bagged model was the best. The model was only the best by a small margin. All the other models performed similarly aside from the pruned model. Each model had a small to zero Test Sensitivity which makes our models impractical for usage. 

Our Decision Tree formula would be as follows:

$$
stroke \sim age + avg\_glucose\_level + smoking\_status
$$

\newpage

## Conclusion

Overall, the predictors most significant in predicting whether or not a stroke may occur based on our best logistic regression model are: age, hypertension, heart_disease and avg_glucose_level whereas for our Bagged Tree indicated avg_glucose_level, bmi, age, and smoking_status. Surprisingly, heart_disease was not one of the significant factors for the tree models contrary to what we would have assumed.
	
Due to the removal of some aspects of the dataset, our models were not able to fit accurately. Removing the rows with NA values resulted in having 250 observations that had a stroke (1) to 180 observations that had a stroke (1). When we fit our models on this cleaned dataset, the 3246 observations that did not have a stroke (0) seems to skew our predictions (there also seems to not be a strong correlation between the predictors for stroke). As a consequence, our models resulted in having values drastically low (test sensitivity) or extremely high (test specificity). The Logistic Regression model outputted an extremely small $R^2$ value of $0.1850374$, indicating that this model is not a good fit for our dataset. Comparatively, the Bootstrap Aggregating model had provided the best overall prediction performance of all decision tree models when looking relatively at the test sensitivity and test specificity. The issues with our models stem from skewed data, the bagged model appears to have a marginal advantage over the logistic regression model (health2.glm) lended by it's test sensitivity indicating it correctly predicts having a stroke more frequently.

## Bibliography

Soriano F. (2021, December). Stroke Prediction Dataset Version 1. 
Retrieved December 2, 2022 from https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset
