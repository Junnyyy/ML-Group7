---
title: "Stroke Analysis Report"
author: "Christine Do, Alyssa Guillory, Johnny Le, Giselle Ruiz, Christopher Turcios, Jimmy Vuong"
date: "2022-12-2"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
```   

\newpage

## Introduction  
With heart disease being the leading cause of death in the United States, we think itâ€™s important to examine the factors that may lead to strokes. Examining the factors for strokes helps predict and create an image of strokes and heart disease. Our data is sourced from Kaggle with a confidential source and is designated for educational purposes only.

### Dataset Information  
Data Variables:   

-   smoking_status: The smoking status of the observation. [Categorical factor w/3 levels]
-   id: The unique identifier for the given observation [numeric]
-   gender: The gender of the observation [Categorical Factor w/3 levels]
-   age: Age of the observation [numeric]
-   hypertension: Whether or not the observation has hypertension [Binary dummy variable]
-   heart_disease: Whether or not the observation has heart disease [Binary dummy variable]
-   ever_married: Whether or not the observation is married [Binary dummy variable]
-   work_type: The type of work the observation participates in [Categorical Factor w/5 levels]
-   Residence_type: The location in which the observation is located [Categorical factor w/2 levels]
-   avg_glucose_level: The average glucose level [numeric]
-   bmi: The body mass index [numeric]
-   smoking_status: The smoking status of the observation. [Categorical factor w/3 levels]
-   stroke: If the observation has had a stroke [Binary dummy variable]

#### Cleaning the data
To use this data set, we first converted variables to their correct representations and then omitted any incomplete observations.

##### Notable fixes:
-   The categorical variable smoking_status was reformatted to remove the level "Unknown", as it was used to represent unavailable data. Any cells that previously had "Unknown" were updated to reflect their unavailable status.
-   The quantitative variable bmi was fixed by converting the data type from character to numeric. This change does not apply to non-numeric cells, so the cells containing "N/A" were changed to reflect their unavailable status in the now numeric column.

```{r, include=FALSE}
library(readr)
# Load in health Data
health <- read_csv("healthcare-dataset-stroke-data.csv",
  col_types = cols(
    gender = col_factor(levels = c("Male", "Female", "Other")),
    hypertension = col_factor(levels = c("0", "1")),
    heart_disease = col_factor(levels = c("0", "1")),
    ever_married = col_factor(levels = c("No", "Yes")),
    work_type = col_factor(
      levels =
        c("children", "Govt_job", "Never_worked", "Private", "Self-employed")
    ),
    Residence_type = col_factor(levels = c("Rural", "Urban")),
    bmi = col_number(),
    smoking_status = col_factor(
      levels =
        c("formerly smoked", "never smoked", "smokes")
    ),
    stroke = col_factor(levels = c("0", "1"))
  ),
  na = c("Unknown", "N/A", "")
)
# Remove ID column
health = health[,-1]
# Omit NA values
health = na.omit(health)
```   
### Main Question
Which factors demonstrate statistical significance in relation to the response variable stroke?

\newpage

## Logistic Regression (Christine Do, Alyssa Guillory, Jimmy Vuong)
We chose to use a logistic regression model for our data because our response variable is qualitative with two classes. It was desirable to have a model predict the probability of a person having a stroke using binary classification. If linear regression had been used to predict the probability of whether or not a person will have a stroke instead, then the model may have predicted Y values outside of our intended range of 0-1. The logistic regression model allows us to enforce this restriction of range.

### Model Formula
Our basic logistic regression formula with all our predictors would be as follows:

```{r log reg model formula, out.width = '100%'}
knitr::include_graphics("logreg.png")
```   

This is the logistic regression formula when we consider all of our variables as predictors for the response variable stroke. However, we automatically excluded id as a predictor, as it is a unique number arbitrarily used to identify patients, therefore we know it does not influence our response variable. Using the glm() function, we created our initial logistic model with stroke being the response and all the other variables as predictors.  
```{r,include=FALSE}
#create model with all predictors
set.seed(500)
health.glm = glm(stroke ~ ., family = "binomial", data=health)
#create model with significant predictors found from doing stepwise backward method on original model
health2.glm = step(health.glm, direction = "backward")
#health2.glm = glm(formula = stroke ~ age + hypertension + heart_disease + avg_glucose_level, 
#    family = "binomial", data = health)

#create a model w/out heart_disease
health3.glm = glm(formula = stroke ~ age + hypertension + avg_glucose_level, family = "binomial", data = health)
```   
```{r}
summary(health.glm)
```  
As the summary for health.glm shows, the predictors: age, hypertension, and avg_glucose_level, show high levels of significance and the predictor heart_disease shows a moderate level of significance. Since our p-values for all four of these predictors are less than $\alpha$=0.1, we believe that these are the most important variables in predicting the probability of having a stoke; To confirm that these variables are the most significant, we used the backwards step() function to see if it would give us the same significant variables and it did.   
```{r,echo=TRUE}
#Model created from backwards step()
summary(health2.glm)
#Model created without heart_disease
summary(health3.glm)
```

From the health2.glm summary we can see that the AIC is 1159.9, which is an improvement from health.glm's AIC of 1173.8, but not by much. With that in mind, we decided to remove the lowest significant predictor (heart_disease) to perform one last model refitting on our data for further comparison. This time, our AIC for the third model (health3.glm) came out to be 1162.1, which is worse than health2.glm, but still slightly better than health.glm.   
$$P(stroke = 1) =$$

$$\frac{exp(-7.632810+0.067773*age+0.568379*hypertension+0.453704*heart\_disease+0.004701*avg\_glucose\_level)}{1 + exp(-7.632810+0.067773*age+0.568379*hypertension+0.453704*heart\_disease+0.004701*avg\_glucose\_level)}$$   
```{r, include=FALSE}
#comparing models

#health.glm info
summary(health.glm)
fit.bic=BIC(health.glm)
fit.stat = cbind(health.glm$null.deviance,health.glm$deviance,(1-(health.glm$deviance/health.glm$null.deviance)),health.glm$aic,fit.bic)
colnames(fit.stat) = c("Null Deviance","Residual Deviance","R^2","AIC","BIC")

#health2.glm info
summary(health2.glm)
health2.bic=BIC(health2.glm)
health2.stat = cbind(health2.glm$null.deviance,health2.glm$deviance,(1-(health2.glm$deviance/health2.glm$null.deviance)),health2.glm$aic,health2.bic)
colnames(health2.stat) = c("Null Deviance","Residual Deviance","R^2","AIC","BIC")

#health3.glm info
summary(health3.glm)
health3.bic=BIC(health3.glm)
health3.stat = cbind(health3.glm$null.deviance,health3.glm$deviance,(1-(health3.glm$deviance/health3.glm$null.deviance)),health3.glm$aic, health3.bic)
colnames(fit.stat) = c("Null Deviance","Residual Deviance","R^2","AIC","BIC")

goodness.fit=rbind(fit.stat,health2.stat,health3.stat)
rownames(goodness.fit) = c("health.glm","health2.glm","health3.glm")
```

Additional information was obtained by using the Goodness of Fit statistical hypothesis test. Given the three models we tested above, we are able to determine each models' null deviance, residual deviance, R\^2 value, AIC value, and BIC value. The lower the residual deviance, AIC, and BIC values, the better the model is able to predict the value of the response variable. As for R\^2, a value closest to 1 is best.

```{r}
print(goodness.fit)
```

```{r, include=FALSE}
##training & testing
test.error = 0
train.error = 0
test.sensitivity = 0
test.specificity = 0
for (i in 1:10) {
  set.seed(i)
  #Select 80% of the data
  sample = sample.int(n = nrow(health),size = round(.80*nrow(health)), replace = FALSE)
  
  train = health[sample,]
  test = health[-sample,]
  #create model on training data
  health.train = glm(formula = stroke ~ age + hypertension + heart_disease + avg_glucose_level, family = "binomial", data = train)
  #summary(health.train)
  
  #predicting based on the model health with the training data
  prediction.train = predict(health.train, type="response")
  predict.stroke.train = ifelse(prediction.train < 0.5,"0","1")
  (conf.mat.train = table(predict.stroke.train,train$stroke))
  
  rows.train = nrow(conf.mat.train)
  if(rows.train < 2) {
    train.error[i] = (conf.mat.train[1,2]/sum(conf.mat.train))
  } else {
    train.error[i] = (conf.mat.train[1,2]+conf.mat.train[2,1])/sum(conf.mat.train)
  }
  
  #predicting based on the model fit with the testing data
  predict.health = predict(health.train, type = "response", newdata = test)
  predict.stroke = ifelse(predict.health< 0.5,"0","1")
  (conf.test = table(predict.stroke,test$stroke))
  
  rows = nrow(conf.test)
  if(rows < 2) {
    test.error[i] = (conf.test[1,2]/sum(conf.test))
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/conf.test[1,1]
  } else if(ncol(conf.test)<1) {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
  else {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = conf.test[2,2]/(conf.test[1,2]+conf.test[2,2])
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
}
train.error.m = mean(train.error)
test.error.m = mean(test.error)
test.sensitivity.m = mean(test.sensitivity)
test.specificity.m = mean(test.specificity)

```   

```{r, echo=FALSE}
cat("Train Error Rate Mean: ", train.error.m)
cat("Test Error Rate Mean: ", test.error.m)
cat("Test Sensitivity Mean: ", test.sensitivity.m)
cat("Test Specificity Mean: ", test.specificity.m)
```  
Between the three models, health2.glm and health3.glm provided the closest fit for our data, with health2.glm having a slight advantage with lower residual deviance and AIC values. Each model's R\^2 value is extremely low, with values being between 0.1 and 0.2, indicating that these models are not a good fit for our data. Even so, we conclude that the best linear regression model for fitting the data and predicting whether or not a stroke will occur is the model health2.glm (predictors: age, hypertension, heart_disease, avg_glucose_level). 

\newpage

## Decision Tree (Johnny Le, Giselle Ruiz, Christopher Turcios)

### Decision Tree Formuala


```{r, message=FALSE, include=FALSE}
#install.packages("tree", repos = "http://cran.us.r-project.org")
#install.packages("randomForest", repos = "http://cran.us.r-project.org")
library(tree)
library(randomForest)
```

### Base Decision Tree

```{r}
# Test random forest model 10 times
test.error = 0
train.error = 0
test.sensitivity = 0
test.specificity = 0

for (i in 1:10) {
  set.seed(i)

  train <- sample(1:nrow(health), nrow(health) / 2 + 0.5)
  health.train <- health[train, ]

  # Create testing data
  health.test <- health[-train, ]


  # Create Decision Tree
  health.tree <- tree(stroke ~ ., data = health.train)

  # Predict using testing Data
  health.pred <- predict(health.tree, health.test, type = "class")
  (conf.test = table(health.pred, health.test$stroke))
  
  rows = nrow(conf.test)
  if(rows < 2) {
    test.error[i] = (conf.test[1,2]/sum(conf.test))
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/conf.test[1,1]
  } else if(ncol(conf.test)<1) {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
  else {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = conf.test[2,2]/(conf.test[1,2]+conf.test[2,2])
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
}

cat("Test Error: ", mean(test.error))
cat("Test Sensitivity: ", mean(test.sensitivity))
cat("Test Specificity: ", mean(test.specificity))

plot(health.tree)
text(health.tree, pretty = 0)
```

### Pruned Decision Tree

```{r}
# Test random forest model 10 times
test.error = 0
train.error = 0
test.sensitivity = 0
test.specificity = 0

for (i in 1:10) {
  set.seed(i)

  train <- sample(1:nrow(health), nrow(health) / 2 + 0.5)
  health.train <- health[train, ]

  # Create testing data
  health.test <- health[-train, ]


  # Prune Decision Tree
  cv.health <- cv.tree(health.tree, FUN = prune.misclass)

  # Predict using Pruned Tree
  health.pruned <- prune.misclass(health.tree, best = 5)
  health.pred.pruned <- predict(health.pruned, health.test, type = "class")
  (conf.test = table(health.pred.pruned, health.test$stroke))
  
  rows = nrow(conf.test)
  if(rows < 2) {
    test.error[i] = (conf.test[1,2]/sum(conf.test))
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/conf.test[1,1]
  } else if(ncol(conf.test)<1) {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
  else {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = conf.test[2,2]/(conf.test[1,2]+conf.test[2,2])
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
}

cat("Test Error: ", mean(test.error))
cat("Test Sensitivity: ", mean(test.sensitivity))
cat("Test Specificity: ", mean(test.specificity))

plot(cv.health$size, cv.health$dev, type = "b")
```

### Bagged Decision Tree

```{r}
# Test random forest model 10 times
test.error = 0
train.error = 0
test.sensitivity = 0
test.specificity = 0

for (i in 1:10) {
  set.seed(i)

  train <- sample(1:nrow(health), nrow(health) / 2 + 0.5)
  health.train <- health[train, ]

  # Create testing data
  health.test <- health[-train, ]


  # Bagged Model
  p <- ncol(health)
  n <- nrow(health)
  B <- 1000
  health.bag <- randomForest(stroke ~ ., data = health.train, mtry = p, ntree = B, importance = TRUE)

  # Predict using Bagged Model
  health.pred.bag <- predict(health.bag, health.test, type = "class")
  (conf.test = table(health.pred.bag, health.test$stroke))
  
  rows = nrow(conf.test)
  if(rows < 2) {
    test.error[i] = (conf.test[1,2]/sum(conf.test))
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/conf.test[1,1]
  } else if(ncol(conf.test)<1) {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
  else {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = conf.test[2,2]/(conf.test[1,2]+conf.test[2,2])
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
}

cat("Test Error: ", mean(test.error))
cat("Test Sensitivity: ", mean(test.sensitivity))
cat("Test Specificity: ", mean(test.specificity))

varImpPlot(health.bag)
health.tree.bag <- tree(stroke ~ ., data = health.train, subset = health.bag$inbag)
plot(health.tree.bag)
text(health.tree.bag, pretty = 0)
```

### Random Forest

```{r}
# Test random forest model 10 times
test.error = 0
train.error = 0
test.sensitivity = 0
test.specificity = 0

for (i in 1:10) {
  set.seed(i)

  train <- sample(1:nrow(health), nrow(health) / 2 + 0.5)
  health.train <- health[train, ]

  # Create testing data
  health.test <- health[-train, ]


  # Random Forest Variables
  STROKE.test <- as.factor(health.train$stroke)
  X.test <- health[-train, -11]
  p <- ncol(health)
  n <- nrow(health)
  B <- 1000

  # Random Forest Model
  health.rf <- randomForest(stroke ~ ., data = health, subset = train, xtest = X.test, ytest = STROKE.test, mtry = sqrt(p), ntree = B, importance = TRUE, keep.forest = TRUE)

  # Predict using Random Forest Model
  health.pred.rf <- predict(health.rf, health.test, type = "class")
  health.stroke.pred <- ifelse(health.pred.rf < 0.5, "0", "1")
  (conf.test = table(health.pred.rf, health.test$stroke))
  
  rows = nrow(conf.test)
  if(rows < 2) {
    test.error[i] = (conf.test[1,2]/sum(conf.test))
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/conf.test[1,1]
  } else if(ncol(conf.test)<1) {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = 0
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
  else {
    test.error[i] = (conf.test[1,2]+conf.test[2,1])/sum(conf.test)
    test.sensitivity[i] = conf.test[2,2]/(conf.test[1,2]+conf.test[2,2])
    test.specificity[i] = conf.test[1,1]/(conf.test[1,1]+conf.test[2,1])
  }
}

cat("Test Error: ", mean(test.error))
cat("Test Sensitivity: ", mean(test.sensitivity))
cat("Test Specificity: ", mean(test.specificity))

varImpPlot(health.rf)
health.tree.rf <- tree(stroke ~ ., data = health.train, subset = health.rf$inbag)
plot(health.tree.rf)
text(health.tree.rf, pretty = 0)
```

\newpage

## Conclusion

\newpage

## Bibliography
